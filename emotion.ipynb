{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0XPkUBZ2aEzu",
        "outputId": "e1f750c8-fa25-45b6-bcc1-0c9d50c0f893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2035 - loss: 1.9940\n",
            "Epoch 1: val_loss improved from inf to 1.84547, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.2039 - loss: 1.9917 - val_accuracy: 0.2740 - val_loss: 1.8455 - learning_rate: 0.0010\n",
            "Epoch 2/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2412 - loss: 1.8288\n",
            "Epoch 2: val_loss did not improve from 1.84547\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.2412 - loss: 1.8290 - val_accuracy: 0.2150 - val_loss: 1.8749 - learning_rate: 0.0010\n",
            "Epoch 3/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2533 - loss: 1.8027\n",
            "Epoch 3: val_loss did not improve from 1.84547\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.2534 - loss: 1.8025 - val_accuracy: 0.2530 - val_loss: 1.8818 - learning_rate: 0.0010\n",
            "Epoch 4/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2907 - loss: 1.7676\n",
            "Epoch 4: val_loss did not improve from 1.84547\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.2900 - loss: 1.7681 - val_accuracy: 0.1890 - val_loss: 1.9399 - learning_rate: 0.0010\n",
            "Epoch 5/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2865 - loss: 1.7592\n",
            "Epoch 5: val_loss improved from 1.84547 to 1.84439, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.2863 - loss: 1.7597 - val_accuracy: 0.2960 - val_loss: 1.8444 - learning_rate: 0.0010\n",
            "Epoch 6/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2920 - loss: 1.7387\n",
            "Epoch 6: val_loss improved from 1.84439 to 1.67298, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2923 - loss: 1.7386 - val_accuracy: 0.3530 - val_loss: 1.6730 - learning_rate: 0.0010\n",
            "Epoch 7/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3038 - loss: 1.7147\n",
            "Epoch 7: val_loss improved from 1.67298 to 1.62385, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.3040 - loss: 1.7148 - val_accuracy: 0.3860 - val_loss: 1.6238 - learning_rate: 0.0010\n",
            "Epoch 8/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3235 - loss: 1.7025\n",
            "Epoch 8: val_loss did not improve from 1.62385\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.3236 - loss: 1.7024 - val_accuracy: 0.3770 - val_loss: 1.6425 - learning_rate: 0.0010\n",
            "Epoch 9/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3394 - loss: 1.6667\n",
            "Epoch 9: val_loss did not improve from 1.62385\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.3394 - loss: 1.6670 - val_accuracy: 0.3390 - val_loss: 1.6474 - learning_rate: 0.0010\n",
            "Epoch 10/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3456 - loss: 1.6476\n",
            "Epoch 10: val_loss did not improve from 1.62385\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3449 - loss: 1.6485 - val_accuracy: 0.3460 - val_loss: 1.7187 - learning_rate: 0.0010\n",
            "Epoch 11/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3438 - loss: 1.6660\n",
            "Epoch 11: val_loss did not improve from 1.62385\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.3438 - loss: 1.6658 - val_accuracy: 0.3700 - val_loss: 1.6527 - learning_rate: 0.0010\n",
            "Epoch 12/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3347 - loss: 1.6617\n",
            "Epoch 12: val_loss did not improve from 1.62385\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.3347 - loss: 1.6617 - val_accuracy: 0.3220 - val_loss: 1.7220 - learning_rate: 0.0010\n",
            "Epoch 13/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3656 - loss: 1.6355\n",
            "Epoch 13: val_loss improved from 1.62385 to 1.57778, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.3653 - loss: 1.6353 - val_accuracy: 0.3930 - val_loss: 1.5778 - learning_rate: 0.0010\n",
            "Epoch 14/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3605 - loss: 1.6118\n",
            "Epoch 14: val_loss did not improve from 1.57778\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3601 - loss: 1.6124 - val_accuracy: 0.4050 - val_loss: 1.5803 - learning_rate: 0.0010\n",
            "Epoch 15/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3495 - loss: 1.6194\n",
            "Epoch 15: val_loss did not improve from 1.57778\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.3497 - loss: 1.6197 - val_accuracy: 0.3680 - val_loss: 1.5924 - learning_rate: 0.0010\n",
            "Epoch 16/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3674 - loss: 1.5843\n",
            "Epoch 16: val_loss did not improve from 1.57778\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.3673 - loss: 1.5845 - val_accuracy: 0.3760 - val_loss: 1.5816 - learning_rate: 0.0010\n",
            "Epoch 17/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3728 - loss: 1.5928\n",
            "Epoch 17: val_loss did not improve from 1.57778\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3728 - loss: 1.5925 - val_accuracy: 0.3880 - val_loss: 1.5980 - learning_rate: 0.0010\n",
            "Epoch 18/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3762 - loss: 1.6078\n",
            "Epoch 18: val_loss improved from 1.57778 to 1.49556, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.3761 - loss: 1.6076 - val_accuracy: 0.4210 - val_loss: 1.4956 - learning_rate: 0.0010\n",
            "Epoch 19/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3977 - loss: 1.5600\n",
            "Epoch 19: val_loss did not improve from 1.49556\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3976 - loss: 1.5603 - val_accuracy: 0.4070 - val_loss: 1.5331 - learning_rate: 0.0010\n",
            "Epoch 20/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3913 - loss: 1.5565\n",
            "Epoch 20: val_loss did not improve from 1.49556\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3910 - loss: 1.5571 - val_accuracy: 0.3810 - val_loss: 1.5592 - learning_rate: 0.0010\n",
            "Epoch 21/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3797 - loss: 1.5457\n",
            "Epoch 21: val_loss improved from 1.49556 to 1.46780, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.3797 - loss: 1.5462 - val_accuracy: 0.4330 - val_loss: 1.4678 - learning_rate: 0.0010\n",
            "Epoch 22/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3869 - loss: 1.5661\n",
            "Epoch 22: val_loss improved from 1.46780 to 1.46731, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.3871 - loss: 1.5657 - val_accuracy: 0.4130 - val_loss: 1.4673 - learning_rate: 0.0010\n",
            "Epoch 23/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3866 - loss: 1.5454\n",
            "Epoch 23: val_loss did not improve from 1.46731\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.3867 - loss: 1.5455 - val_accuracy: 0.4070 - val_loss: 1.5737 - learning_rate: 0.0010\n",
            "Epoch 24/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3854 - loss: 1.5451\n",
            "Epoch 24: val_loss did not improve from 1.46731\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.3856 - loss: 1.5452 - val_accuracy: 0.4510 - val_loss: 1.4904 - learning_rate: 0.0010\n",
            "Epoch 25/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4052 - loss: 1.5236\n",
            "Epoch 25: val_loss improved from 1.46731 to 1.44317, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4048 - loss: 1.5241 - val_accuracy: 0.4310 - val_loss: 1.4432 - learning_rate: 0.0010\n",
            "Epoch 26/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4128 - loss: 1.5240\n",
            "Epoch 26: val_loss did not improve from 1.44317\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.4122 - loss: 1.5242 - val_accuracy: 0.4250 - val_loss: 1.4553 - learning_rate: 0.0010\n",
            "Epoch 27/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4034 - loss: 1.5139\n",
            "Epoch 27: val_loss improved from 1.44317 to 1.41696, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4034 - loss: 1.5145 - val_accuracy: 0.4500 - val_loss: 1.4170 - learning_rate: 0.0010\n",
            "Epoch 28/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4148 - loss: 1.5024\n",
            "Epoch 28: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.4148 - loss: 1.5025 - val_accuracy: 0.4560 - val_loss: 1.4673 - learning_rate: 0.0010\n",
            "Epoch 29/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4004 - loss: 1.5083\n",
            "Epoch 29: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4009 - loss: 1.5085 - val_accuracy: 0.4290 - val_loss: 1.4875 - learning_rate: 0.0010\n",
            "Epoch 30/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4089 - loss: 1.5075\n",
            "Epoch 30: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4091 - loss: 1.5070 - val_accuracy: 0.4640 - val_loss: 1.4267 - learning_rate: 0.0010\n",
            "Epoch 31/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4103 - loss: 1.4895\n",
            "Epoch 31: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4103 - loss: 1.4901 - val_accuracy: 0.4320 - val_loss: 1.4760 - learning_rate: 0.0010\n",
            "Epoch 32/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4209 - loss: 1.5035\n",
            "Epoch 32: val_loss improved from 1.41696 to 1.40694, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.4208 - loss: 1.5035 - val_accuracy: 0.4510 - val_loss: 1.4069 - learning_rate: 0.0010\n",
            "Epoch 33/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4247 - loss: 1.4880\n",
            "Epoch 33: val_loss did not improve from 1.40694\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4247 - loss: 1.4880 - val_accuracy: 0.4500 - val_loss: 1.4788 - learning_rate: 0.0010\n",
            "Epoch 34/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4161 - loss: 1.4818\n",
            "Epoch 34: val_loss improved from 1.40694 to 1.40282, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4161 - loss: 1.4818 - val_accuracy: 0.4490 - val_loss: 1.4028 - learning_rate: 0.0010\n",
            "Epoch 35/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4380 - loss: 1.4430\n",
            "Epoch 35: val_loss improved from 1.40282 to 1.38539, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4376 - loss: 1.4443 - val_accuracy: 0.4650 - val_loss: 1.3854 - learning_rate: 0.0010\n",
            "Epoch 36/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4117 - loss: 1.4874\n",
            "Epoch 36: val_loss did not improve from 1.38539\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4124 - loss: 1.4865 - val_accuracy: 0.4360 - val_loss: 1.4131 - learning_rate: 0.0010\n",
            "Epoch 37/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4329 - loss: 1.4556\n",
            "Epoch 37: val_loss did not improve from 1.38539\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.4328 - loss: 1.4561 - val_accuracy: 0.4570 - val_loss: 1.4019 - learning_rate: 0.0010\n",
            "Epoch 38/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4373 - loss: 1.4678\n",
            "Epoch 38: val_loss did not improve from 1.38539\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4371 - loss: 1.4677 - val_accuracy: 0.4400 - val_loss: 1.4989 - learning_rate: 0.0010\n",
            "Epoch 39/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4338 - loss: 1.4432\n",
            "Epoch 39: val_loss did not improve from 1.38539\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4339 - loss: 1.4434 - val_accuracy: 0.4650 - val_loss: 1.4162 - learning_rate: 0.0010\n",
            "Epoch 40/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4286 - loss: 1.4719\n",
            "Epoch 40: val_loss improved from 1.38539 to 1.35952, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4287 - loss: 1.4718 - val_accuracy: 0.4890 - val_loss: 1.3595 - learning_rate: 0.0010\n",
            "Epoch 41/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4316 - loss: 1.4620\n",
            "Epoch 41: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4316 - loss: 1.4620 - val_accuracy: 0.4690 - val_loss: 1.4240 - learning_rate: 0.0010\n",
            "Epoch 42/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4148 - loss: 1.4640\n",
            "Epoch 42: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4157 - loss: 1.4635 - val_accuracy: 0.4490 - val_loss: 1.4301 - learning_rate: 0.0010\n",
            "Epoch 43/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4251 - loss: 1.4551\n",
            "Epoch 43: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4252 - loss: 1.4550 - val_accuracy: 0.4730 - val_loss: 1.3615 - learning_rate: 0.0010\n",
            "Epoch 44/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4406 - loss: 1.4418\n",
            "Epoch 44: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4406 - loss: 1.4420 - val_accuracy: 0.4670 - val_loss: 1.3789 - learning_rate: 0.0010\n",
            "Epoch 45/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4485 - loss: 1.4224\n",
            "Epoch 45: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.4482 - loss: 1.4234 - val_accuracy: 0.4740 - val_loss: 1.3940 - learning_rate: 0.0010\n",
            "Epoch 46/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4497 - loss: 1.4381\n",
            "Epoch 46: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.4496 - loss: 1.4379 - val_accuracy: 0.4600 - val_loss: 1.3608 - learning_rate: 0.0010\n",
            "Epoch 47/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4655 - loss: 1.3887\n",
            "Epoch 47: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4653 - loss: 1.3892 - val_accuracy: 0.4570 - val_loss: 1.3838 - learning_rate: 0.0010\n",
            "Epoch 48/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4508 - loss: 1.4286\n",
            "Epoch 48: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4507 - loss: 1.4288 - val_accuracy: 0.4440 - val_loss: 1.3753 - learning_rate: 0.0010\n",
            "Epoch 49/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4501 - loss: 1.4166\n",
            "Epoch 49: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.4499 - loss: 1.4166 - val_accuracy: 0.4640 - val_loss: 1.3656 - learning_rate: 0.0010\n",
            "Epoch 50/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4365 - loss: 1.4338\n",
            "Epoch 50: val_loss did not improve from 1.35952\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4368 - loss: 1.4337 - val_accuracy: 0.4500 - val_loss: 1.4255 - learning_rate: 0.0010\n",
            "Epoch 51/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4555 - loss: 1.4338\n",
            "Epoch 51: val_loss improved from 1.35952 to 1.32641, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4554 - loss: 1.4336 - val_accuracy: 0.4760 - val_loss: 1.3264 - learning_rate: 0.0010\n",
            "Epoch 52/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4533 - loss: 1.4108\n",
            "Epoch 52: val_loss did not improve from 1.32641\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4532 - loss: 1.4113 - val_accuracy: 0.4620 - val_loss: 1.3819 - learning_rate: 0.0010\n",
            "Epoch 53/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4482 - loss: 1.4164\n",
            "Epoch 53: val_loss did not improve from 1.32641\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.4481 - loss: 1.4165 - val_accuracy: 0.4150 - val_loss: 1.5903 - learning_rate: 0.0010\n",
            "Epoch 54/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4418 - loss: 1.4329\n",
            "Epoch 54: val_loss did not improve from 1.32641\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4424 - loss: 1.4321 - val_accuracy: 0.4780 - val_loss: 1.3454 - learning_rate: 0.0010\n",
            "Epoch 55/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4470 - loss: 1.4175\n",
            "Epoch 55: val_loss improved from 1.32641 to 1.32562, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4474 - loss: 1.4170 - val_accuracy: 0.5000 - val_loss: 1.3256 - learning_rate: 0.0010\n",
            "Epoch 56/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4628 - loss: 1.4088\n",
            "Epoch 56: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.4626 - loss: 1.4090 - val_accuracy: 0.4730 - val_loss: 1.3553 - learning_rate: 0.0010\n",
            "Epoch 57/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4735 - loss: 1.4104\n",
            "Epoch 57: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4735 - loss: 1.4102 - val_accuracy: 0.4780 - val_loss: 1.3298 - learning_rate: 0.0010\n",
            "Epoch 58/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4568 - loss: 1.3892\n",
            "Epoch 58: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.4566 - loss: 1.3902 - val_accuracy: 0.4480 - val_loss: 1.4192 - learning_rate: 0.0010\n",
            "Epoch 59/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4642 - loss: 1.3917\n",
            "Epoch 59: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.4640 - loss: 1.3918 - val_accuracy: 0.4680 - val_loss: 1.3423 - learning_rate: 0.0010\n",
            "Epoch 60/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4547 - loss: 1.3861\n",
            "Epoch 60: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4548 - loss: 1.3864 - val_accuracy: 0.4820 - val_loss: 1.3405 - learning_rate: 0.0010\n",
            "Epoch 61/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4713 - loss: 1.3613\n",
            "Epoch 61: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4709 - loss: 1.3622 - val_accuracy: 0.4650 - val_loss: 1.3304 - learning_rate: 0.0010\n",
            "Epoch 62/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4699 - loss: 1.3915\n",
            "Epoch 62: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4699 - loss: 1.3915 - val_accuracy: 0.4950 - val_loss: 1.3449 - learning_rate: 0.0010\n",
            "Epoch 63/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4448 - loss: 1.4028\n",
            "Epoch 63: val_loss did not improve from 1.32562\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4452 - loss: 1.4025 - val_accuracy: 0.4850 - val_loss: 1.3817 - learning_rate: 0.0010\n",
            "Epoch 64/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4599 - loss: 1.3904\n",
            "Epoch 64: val_loss improved from 1.32562 to 1.30982, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.4600 - loss: 1.3902 - val_accuracy: 0.4950 - val_loss: 1.3098 - learning_rate: 0.0010\n",
            "Epoch 65/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4708 - loss: 1.3598\n",
            "Epoch 65: val_loss did not improve from 1.30982\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.4707 - loss: 1.3602 - val_accuracy: 0.4870 - val_loss: 1.3301 - learning_rate: 0.0010\n",
            "Epoch 66/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4783 - loss: 1.3447\n",
            "Epoch 66: val_loss did not improve from 1.30982\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.4781 - loss: 1.3451 - val_accuracy: 0.4650 - val_loss: 1.3966 - learning_rate: 0.0010\n",
            "Epoch 67/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4832 - loss: 1.3524\n",
            "Epoch 67: val_loss did not improve from 1.30982\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.4827 - loss: 1.3535 - val_accuracy: 0.4690 - val_loss: 1.3468 - learning_rate: 0.0010\n",
            "Epoch 68/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4735 - loss: 1.3565\n",
            "Epoch 68: val_loss did not improve from 1.30982\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4731 - loss: 1.3572 - val_accuracy: 0.4970 - val_loss: 1.3218 - learning_rate: 0.0010\n",
            "Epoch 69/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4835 - loss: 1.3564\n",
            "Epoch 69: val_loss improved from 1.30982 to 1.30885, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step - accuracy: 0.4833 - loss: 1.3566 - val_accuracy: 0.4950 - val_loss: 1.3088 - learning_rate: 0.0010\n",
            "Epoch 70/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4814 - loss: 1.3527\n",
            "Epoch 70: val_loss improved from 1.30885 to 1.28682, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.4811 - loss: 1.3532 - val_accuracy: 0.5050 - val_loss: 1.2868 - learning_rate: 0.0010\n",
            "Epoch 71/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4876 - loss: 1.3506\n",
            "Epoch 71: val_loss did not improve from 1.28682\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4874 - loss: 1.3508 - val_accuracy: 0.5050 - val_loss: 1.3044 - learning_rate: 0.0010\n",
            "Epoch 72/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4701 - loss: 1.3716\n",
            "Epoch 72: val_loss did not improve from 1.28682\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4701 - loss: 1.3716 - val_accuracy: 0.4770 - val_loss: 1.3370 - learning_rate: 0.0010\n",
            "Epoch 73/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4849 - loss: 1.3677\n",
            "Epoch 73: val_loss did not improve from 1.28682\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.4847 - loss: 1.3677 - val_accuracy: 0.5010 - val_loss: 1.3076 - learning_rate: 0.0010\n",
            "Epoch 74/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4669 - loss: 1.3525\n",
            "Epoch 74: val_loss improved from 1.28682 to 1.26842, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4670 - loss: 1.3525 - val_accuracy: 0.5100 - val_loss: 1.2684 - learning_rate: 0.0010\n",
            "Epoch 75/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4879 - loss: 1.3404\n",
            "Epoch 75: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4876 - loss: 1.3408 - val_accuracy: 0.5040 - val_loss: 1.2735 - learning_rate: 0.0010\n",
            "Epoch 76/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4741 - loss: 1.3716\n",
            "Epoch 76: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4743 - loss: 1.3714 - val_accuracy: 0.4920 - val_loss: 1.3222 - learning_rate: 0.0010\n",
            "Epoch 77/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4577 - loss: 1.3761\n",
            "Epoch 77: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.4582 - loss: 1.3758 - val_accuracy: 0.4760 - val_loss: 1.3572 - learning_rate: 0.0010\n",
            "Epoch 78/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4793 - loss: 1.3502\n",
            "Epoch 78: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4794 - loss: 1.3499 - val_accuracy: 0.4840 - val_loss: 1.3375 - learning_rate: 0.0010\n",
            "Epoch 79/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4713 - loss: 1.3655\n",
            "Epoch 79: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4714 - loss: 1.3654 - val_accuracy: 0.5030 - val_loss: 1.3306 - learning_rate: 0.0010\n",
            "Epoch 80/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4704 - loss: 1.3380\n",
            "Epoch 80: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.4704 - loss: 1.3382 - val_accuracy: 0.5100 - val_loss: 1.3035 - learning_rate: 0.0010\n",
            "Epoch 81/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4663 - loss: 1.3593\n",
            "Epoch 81: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.4664 - loss: 1.3593 - val_accuracy: 0.4990 - val_loss: 1.3316 - learning_rate: 0.0010\n",
            "Epoch 82/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4918 - loss: 1.3111\n",
            "Epoch 82: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4913 - loss: 1.3123 - val_accuracy: 0.4840 - val_loss: 1.3430 - learning_rate: 0.0010\n",
            "Epoch 83/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4777 - loss: 1.3441\n",
            "Epoch 83: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4777 - loss: 1.3440 - val_accuracy: 0.5020 - val_loss: 1.3379 - learning_rate: 0.0010\n",
            "Epoch 84/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4871 - loss: 1.3412\n",
            "Epoch 84: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4869 - loss: 1.3414 - val_accuracy: 0.4960 - val_loss: 1.3130 - learning_rate: 0.0010\n",
            "Epoch 85/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4848 - loss: 1.3285\n",
            "Epoch 85: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4848 - loss: 1.3285 - val_accuracy: 0.5120 - val_loss: 1.2865 - learning_rate: 0.0010\n",
            "Epoch 86/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4798 - loss: 1.3316\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 86: val_loss did not improve from 1.26842\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.4797 - loss: 1.3318 - val_accuracy: 0.4920 - val_loss: 1.2952 - learning_rate: 0.0010\n",
            "Epoch 87/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5042 - loss: 1.3047\n",
            "Epoch 87: val_loss improved from 1.26842 to 1.25726, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5042 - loss: 1.3047 - val_accuracy: 0.5220 - val_loss: 1.2573 - learning_rate: 1.0000e-04\n",
            "Epoch 88/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4962 - loss: 1.2935\n",
            "Epoch 88: val_loss improved from 1.25726 to 1.25686, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4962 - loss: 1.2939 - val_accuracy: 0.5080 - val_loss: 1.2569 - learning_rate: 1.0000e-04\n",
            "Epoch 89/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5028 - loss: 1.2945\n",
            "Epoch 89: val_loss improved from 1.25686 to 1.25526, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5027 - loss: 1.2951 - val_accuracy: 0.5130 - val_loss: 1.2553 - learning_rate: 1.0000e-04\n",
            "Epoch 90/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5162 - loss: 1.2736\n",
            "Epoch 90: val_loss improved from 1.25526 to 1.25216, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5157 - loss: 1.2745 - val_accuracy: 0.5060 - val_loss: 1.2522 - learning_rate: 1.0000e-04\n",
            "Epoch 91/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5005 - loss: 1.2970\n",
            "Epoch 91: val_loss improved from 1.25216 to 1.25156, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.5005 - loss: 1.2970 - val_accuracy: 0.5140 - val_loss: 1.2516 - learning_rate: 1.0000e-04\n",
            "Epoch 92/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5204 - loss: 1.2637\n",
            "Epoch 92: val_loss improved from 1.25156 to 1.24935, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5202 - loss: 1.2644 - val_accuracy: 0.5110 - val_loss: 1.2494 - learning_rate: 1.0000e-04\n",
            "Epoch 93/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5101 - loss: 1.2853\n",
            "Epoch 93: val_loss improved from 1.24935 to 1.24280, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5102 - loss: 1.2849 - val_accuracy: 0.5200 - val_loss: 1.2428 - learning_rate: 1.0000e-04\n",
            "Epoch 94/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5108 - loss: 1.2854\n",
            "Epoch 94: val_loss did not improve from 1.24280\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5108 - loss: 1.2854 - val_accuracy: 0.5110 - val_loss: 1.2503 - learning_rate: 1.0000e-04\n",
            "Epoch 95/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5002 - loss: 1.2708\n",
            "Epoch 95: val_loss did not improve from 1.24280\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.5000 - loss: 1.2714 - val_accuracy: 0.5110 - val_loss: 1.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 96/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4970 - loss: 1.2944\n",
            "Epoch 96: val_loss did not improve from 1.24280\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4968 - loss: 1.2945 - val_accuracy: 0.5140 - val_loss: 1.2460 - learning_rate: 1.0000e-04\n",
            "Epoch 97/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5110 - loss: 1.2746\n",
            "Epoch 97: val_loss did not improve from 1.24280\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.5110 - loss: 1.2750 - val_accuracy: 0.5080 - val_loss: 1.2430 - learning_rate: 1.0000e-04\n",
            "Epoch 98/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5082 - loss: 1.2539\n",
            "Epoch 98: val_loss did not improve from 1.24280\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5082 - loss: 1.2541 - val_accuracy: 0.5140 - val_loss: 1.2434 - learning_rate: 1.0000e-04\n",
            "Epoch 99/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4982 - loss: 1.2961\n",
            "Epoch 99: val_loss improved from 1.24280 to 1.24221, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.4983 - loss: 1.2958 - val_accuracy: 0.5140 - val_loss: 1.2422 - learning_rate: 1.0000e-04\n",
            "Epoch 100/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5054 - loss: 1.2721\n",
            "Epoch 100: val_loss did not improve from 1.24221\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.5055 - loss: 1.2721 - val_accuracy: 0.5150 - val_loss: 1.2450 - learning_rate: 1.0000e-04\n",
            "Epoch 101/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5108 - loss: 1.2794\n",
            "Epoch 101: val_loss improved from 1.24221 to 1.24146, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5108 - loss: 1.2794 - val_accuracy: 0.5130 - val_loss: 1.2415 - learning_rate: 1.0000e-04\n",
            "Epoch 102/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5038 - loss: 1.2814\n",
            "Epoch 102: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5042 - loss: 1.2808 - val_accuracy: 0.5150 - val_loss: 1.2455 - learning_rate: 1.0000e-04\n",
            "Epoch 103/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5126 - loss: 1.2674\n",
            "Epoch 103: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5126 - loss: 1.2676 - val_accuracy: 0.5130 - val_loss: 1.2433 - learning_rate: 1.0000e-04\n",
            "Epoch 104/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4967 - loss: 1.3024\n",
            "Epoch 104: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.4968 - loss: 1.3023 - val_accuracy: 0.5120 - val_loss: 1.2486 - learning_rate: 1.0000e-04\n",
            "Epoch 105/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5091 - loss: 1.2566\n",
            "Epoch 105: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.5090 - loss: 1.2570 - val_accuracy: 0.5220 - val_loss: 1.2427 - learning_rate: 1.0000e-04\n",
            "Epoch 106/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5171 - loss: 1.2678\n",
            "Epoch 106: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.5169 - loss: 1.2680 - val_accuracy: 0.5200 - val_loss: 1.2473 - learning_rate: 1.0000e-04\n",
            "Epoch 107/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4907 - loss: 1.2920\n",
            "Epoch 107: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.4908 - loss: 1.2918 - val_accuracy: 0.5200 - val_loss: 1.2438 - learning_rate: 1.0000e-04\n",
            "Epoch 108/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5186 - loss: 1.2754\n",
            "Epoch 108: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5186 - loss: 1.2754 - val_accuracy: 0.5120 - val_loss: 1.2507 - learning_rate: 1.0000e-04\n",
            "Epoch 109/200\n",
            "\u001b[1m122/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5048 - loss: 1.2832\n",
            "Epoch 109: val_loss did not improve from 1.24146\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5048 - loss: 1.2832 - val_accuracy: 0.5120 - val_loss: 1.2462 - learning_rate: 1.0000e-04\n",
            "Epoch 110/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5185 - loss: 1.2493\n",
            "Epoch 110: val_loss improved from 1.24146 to 1.23724, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.5180 - loss: 1.2503 - val_accuracy: 0.5150 - val_loss: 1.2372 - learning_rate: 1.0000e-04\n",
            "Epoch 111/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4979 - loss: 1.2780\n",
            "Epoch 111: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4980 - loss: 1.2778 - val_accuracy: 0.5170 - val_loss: 1.2489 - learning_rate: 1.0000e-04\n",
            "Epoch 112/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5064 - loss: 1.2639\n",
            "Epoch 112: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5064 - loss: 1.2639 - val_accuracy: 0.5170 - val_loss: 1.2450 - learning_rate: 1.0000e-04\n",
            "Epoch 113/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5219 - loss: 1.2482\n",
            "Epoch 113: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5217 - loss: 1.2484 - val_accuracy: 0.5180 - val_loss: 1.2450 - learning_rate: 1.0000e-04\n",
            "Epoch 114/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5146 - loss: 1.2482\n",
            "Epoch 114: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5147 - loss: 1.2482 - val_accuracy: 0.5140 - val_loss: 1.2438 - learning_rate: 1.0000e-04\n",
            "Epoch 115/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5224 - loss: 1.2628\n",
            "Epoch 115: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5222 - loss: 1.2631 - val_accuracy: 0.5110 - val_loss: 1.2410 - learning_rate: 1.0000e-04\n",
            "Epoch 116/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5225 - loss: 1.2634\n",
            "Epoch 116: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5223 - loss: 1.2635 - val_accuracy: 0.5200 - val_loss: 1.2377 - learning_rate: 1.0000e-04\n",
            "Epoch 117/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4985 - loss: 1.2838\n",
            "Epoch 117: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4985 - loss: 1.2837 - val_accuracy: 0.5160 - val_loss: 1.2415 - learning_rate: 1.0000e-04\n",
            "Epoch 118/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5172 - loss: 1.2787\n",
            "Epoch 118: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5171 - loss: 1.2786 - val_accuracy: 0.5220 - val_loss: 1.2422 - learning_rate: 1.0000e-04\n",
            "Epoch 119/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5053 - loss: 1.2452\n",
            "Epoch 119: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5055 - loss: 1.2455 - val_accuracy: 0.5140 - val_loss: 1.2413 - learning_rate: 1.0000e-04\n",
            "Epoch 120/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4986 - loss: 1.2811\n",
            "Epoch 120: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4988 - loss: 1.2808 - val_accuracy: 0.5210 - val_loss: 1.2545 - learning_rate: 1.0000e-04\n",
            "Epoch 121/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5094 - loss: 1.2527\n",
            "Epoch 121: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5095 - loss: 1.2531 - val_accuracy: 0.5160 - val_loss: 1.2380 - learning_rate: 1.0000e-04\n",
            "Epoch 122/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4991 - loss: 1.2952\n",
            "Epoch 122: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 122: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.4994 - loss: 1.2946 - val_accuracy: 0.5160 - val_loss: 1.2430 - learning_rate: 1.0000e-04\n",
            "Epoch 123/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5251 - loss: 1.2465\n",
            "Epoch 123: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5249 - loss: 1.2467 - val_accuracy: 0.5160 - val_loss: 1.2399 - learning_rate: 1.0000e-05\n",
            "Epoch 124/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5259 - loss: 1.2445\n",
            "Epoch 124: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.5258 - loss: 1.2448 - val_accuracy: 0.5130 - val_loss: 1.2388 - learning_rate: 1.0000e-05\n",
            "Epoch 125/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5072 - loss: 1.2784\n",
            "Epoch 125: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5073 - loss: 1.2780 - val_accuracy: 0.5140 - val_loss: 1.2387 - learning_rate: 1.0000e-05\n",
            "Epoch 126/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4960 - loss: 1.2812\n",
            "Epoch 126: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.4961 - loss: 1.2812 - val_accuracy: 0.5150 - val_loss: 1.2389 - learning_rate: 1.0000e-05\n",
            "Epoch 127/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5192 - loss: 1.2716\n",
            "Epoch 127: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5191 - loss: 1.2714 - val_accuracy: 0.5150 - val_loss: 1.2389 - learning_rate: 1.0000e-05\n",
            "Epoch 128/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5040 - loss: 1.3010\n",
            "Epoch 128: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5040 - loss: 1.3007 - val_accuracy: 0.5200 - val_loss: 1.2396 - learning_rate: 1.0000e-05\n",
            "Epoch 129/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5143 - loss: 1.2526\n",
            "Epoch 129: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.5142 - loss: 1.2527 - val_accuracy: 0.5160 - val_loss: 1.2385 - learning_rate: 1.0000e-05\n",
            "Epoch 130/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5117 - loss: 1.2609\n",
            "Epoch 130: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5117 - loss: 1.2610 - val_accuracy: 0.5180 - val_loss: 1.2400 - learning_rate: 1.0000e-05\n",
            "Epoch 131/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5108 - loss: 1.2541\n",
            "Epoch 131: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5111 - loss: 1.2539 - val_accuracy: 0.5200 - val_loss: 1.2400 - learning_rate: 1.0000e-05\n",
            "Epoch 132/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5086 - loss: 1.2619\n",
            "Epoch 132: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5086 - loss: 1.2620 - val_accuracy: 0.5200 - val_loss: 1.2405 - learning_rate: 1.0000e-05\n",
            "Epoch 133/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4873 - loss: 1.2877\n",
            "Epoch 133: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.4874 - loss: 1.2877 - val_accuracy: 0.5210 - val_loss: 1.2403 - learning_rate: 1.0000e-05\n",
            "Epoch 134/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5202 - loss: 1.2375\n",
            "Epoch 134: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 134: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5196 - loss: 1.2387 - val_accuracy: 0.5190 - val_loss: 1.2403 - learning_rate: 1.0000e-05\n",
            "Epoch 135/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.4935 - loss: 1.2785\n",
            "Epoch 135: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.4937 - loss: 1.2783 - val_accuracy: 0.5210 - val_loss: 1.2399 - learning_rate: 1.0000e-06\n",
            "Epoch 136/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5095 - loss: 1.2611\n",
            "Epoch 136: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5095 - loss: 1.2612 - val_accuracy: 0.5200 - val_loss: 1.2397 - learning_rate: 1.0000e-06\n",
            "Epoch 137/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5299 - loss: 1.2272\n",
            "Epoch 137: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5298 - loss: 1.2277 - val_accuracy: 0.5200 - val_loss: 1.2402 - learning_rate: 1.0000e-06\n",
            "Epoch 138/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5003 - loss: 1.2888\n",
            "Epoch 138: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.5006 - loss: 1.2884 - val_accuracy: 0.5180 - val_loss: 1.2399 - learning_rate: 1.0000e-06\n",
            "Epoch 139/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5168 - loss: 1.2605\n",
            "Epoch 139: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5167 - loss: 1.2605 - val_accuracy: 0.5180 - val_loss: 1.2401 - learning_rate: 1.0000e-06\n",
            "Epoch 140/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5352 - loss: 1.2206\n",
            "Epoch 140: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5351 - loss: 1.2208 - val_accuracy: 0.5180 - val_loss: 1.2405 - learning_rate: 1.0000e-06\n",
            "Epoch 141/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4959 - loss: 1.2786\n",
            "Epoch 141: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.4961 - loss: 1.2784 - val_accuracy: 0.5180 - val_loss: 1.2405 - learning_rate: 1.0000e-06\n",
            "Epoch 142/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5305 - loss: 1.2581\n",
            "Epoch 142: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.5303 - loss: 1.2580 - val_accuracy: 0.5160 - val_loss: 1.2400 - learning_rate: 1.0000e-06\n",
            "Epoch 143/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5075 - loss: 1.2535\n",
            "Epoch 143: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5075 - loss: 1.2536 - val_accuracy: 0.5200 - val_loss: 1.2400 - learning_rate: 1.0000e-06\n",
            "Epoch 144/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5176 - loss: 1.2795\n",
            "Epoch 144: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5175 - loss: 1.2792 - val_accuracy: 0.5190 - val_loss: 1.2400 - learning_rate: 1.0000e-06\n",
            "Epoch 145/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5328 - loss: 1.2502\n",
            "Epoch 145: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.5324 - loss: 1.2504 - val_accuracy: 0.5170 - val_loss: 1.2398 - learning_rate: 1.0000e-06\n",
            "Epoch 146/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5173 - loss: 1.2484\n",
            "Epoch 146: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "\n",
            "Epoch 146: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5172 - loss: 1.2487 - val_accuracy: 0.5190 - val_loss: 1.2396 - learning_rate: 1.0000e-06\n",
            "Epoch 147/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5072 - loss: 1.2881\n",
            "Epoch 147: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.5074 - loss: 1.2872 - val_accuracy: 0.5180 - val_loss: 1.2397 - learning_rate: 1.0000e-07\n",
            "Epoch 148/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5085 - loss: 1.2734\n",
            "Epoch 148: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5085 - loss: 1.2731 - val_accuracy: 0.5200 - val_loss: 1.2398 - learning_rate: 1.0000e-07\n",
            "Epoch 149/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5201 - loss: 1.2308\n",
            "Epoch 149: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5197 - loss: 1.2316 - val_accuracy: 0.5190 - val_loss: 1.2404 - learning_rate: 1.0000e-07\n",
            "Epoch 150/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5162 - loss: 1.2717\n",
            "Epoch 150: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.5165 - loss: 1.2711 - val_accuracy: 0.5170 - val_loss: 1.2408 - learning_rate: 1.0000e-07\n",
            "Epoch 151/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5247 - loss: 1.2373\n",
            "Epoch 151: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5247 - loss: 1.2373 - val_accuracy: 0.5170 - val_loss: 1.2406 - learning_rate: 1.0000e-07\n",
            "Epoch 152/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5108 - loss: 1.2753\n",
            "Epoch 152: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.5109 - loss: 1.2751 - val_accuracy: 0.5180 - val_loss: 1.2406 - learning_rate: 1.0000e-07\n",
            "Epoch 153/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5110 - loss: 1.2551\n",
            "Epoch 153: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.5110 - loss: 1.2549 - val_accuracy: 0.5180 - val_loss: 1.2397 - learning_rate: 1.0000e-07\n",
            "Epoch 154/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5122 - loss: 1.2659\n",
            "Epoch 154: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.5123 - loss: 1.2659 - val_accuracy: 0.5170 - val_loss: 1.2403 - learning_rate: 1.0000e-07\n",
            "Epoch 155/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5116 - loss: 1.2477\n",
            "Epoch 155: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5114 - loss: 1.2481 - val_accuracy: 0.5150 - val_loss: 1.2405 - learning_rate: 1.0000e-07\n",
            "Epoch 156/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5138 - loss: 1.2711\n",
            "Epoch 156: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5139 - loss: 1.2708 - val_accuracy: 0.5180 - val_loss: 1.2403 - learning_rate: 1.0000e-07\n",
            "Epoch 157/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5176 - loss: 1.2604\n",
            "Epoch 157: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.5174 - loss: 1.2605 - val_accuracy: 0.5170 - val_loss: 1.2404 - learning_rate: 1.0000e-07\n",
            "Epoch 158/200\n",
            "\u001b[1m121/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5141 - loss: 1.2495\n",
            "Epoch 158: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "\n",
            "Epoch 158: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5137 - loss: 1.2502 - val_accuracy: 0.5190 - val_loss: 1.2395 - learning_rate: 1.0000e-07\n",
            "Epoch 159/200\n",
            "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5058 - loss: 1.2715\n",
            "Epoch 159: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.5059 - loss: 1.2716 - val_accuracy: 0.5200 - val_loss: 1.2402 - learning_rate: 1.0000e-08\n",
            "Epoch 160/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4980 - loss: 1.2888\n",
            "Epoch 160: val_loss did not improve from 1.23724\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4980 - loss: 1.2887 - val_accuracy: 0.5200 - val_loss: 1.2406 - learning_rate: 1.0000e-08\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "Predictions saved to mobilenet_predictions.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, BatchNormalization, Activation, AveragePooling2D, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "num_classes = 7\n",
        "patience = 50\n",
        "input_shape = (48, 48, 1)  # The input image is 48x48\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(train_file, test_file):\n",
        "    train_data = pd.read_csv('/content/train_dataset.csv.zip')\n",
        "    test_data = pd.read_csv('/content/train_dataset.csv.zip')\n",
        "    return train_data, test_data\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data(data, is_train=True):\n",
        "    X = np.array([np.fromstring(pixels, dtype=int, sep=' ').reshape(48, 48, 1) for pixels in data['pixels']])\n",
        "    X = X / 255.0  # Normalize the pixel values\n",
        "\n",
        "    if is_train:\n",
        "        y = to_categorical(data['emotion'], num_classes=num_classes)\n",
        "        return X, y\n",
        "    return X\n",
        "\n",
        "# Build the model\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Load and preprocess data\n",
        "train_file = '/path_to_your/train.csv'\n",
        "test_file = '/path_to_your/test.csv'\n",
        "train_data, test_data = load_data(train_file, test_file)\n",
        "\n",
        "# Preprocess train and test datasets\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test = preprocess_data(test_data, is_train=False)\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data generator with augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Compile and train the model\n",
        "model = build_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "csv_logger = CSVLogger('training.log')\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience / 4), verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "callbacks = [csv_logger, early_stop, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    data_generator.flow(X_train, y_train, batch_size=batch_size),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=num_epochs,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = model.predict(X_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "test_data['emotion'] = test_predicted_labels\n",
        "test_data[['id', 'emotion']].to_csv('mobilenet_predictions.csv', index=False)\n",
        "print(\"Predictions saved to mobilenet_predictions.csv\")\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "def plot_training(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function\n",
        "plot_training(history)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "def load_test_data(test_file):\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    return test_data\n",
        "\n",
        "# Data preprocessing function for test data\n",
        "def preprocess_test_data(data):\n",
        "    X = np.array([np.fromstring(pixels, dtype=int, sep=' ').reshape(48, 48, 1) for pixels in data['pixels']])\n",
        "    X = X / 255.0  # Normalize the pixel values\n",
        "    return X\n",
        "\n",
        "# Provide the path for the test CSV file\n",
        "test_file = '/content/test_dataset.csv.zip'\n",
        "\n",
        "# Load and preprocess the test dataset\n",
        "test_data = load_test_data(test_file)\n",
        "X_test = preprocess_test_data(test_data)\n",
        "\n",
        "# Load the best model saved during training\n",
        "best_model = load_model('best_model.keras')\n",
        "\n",
        "# Initialize default test loss and accuracy\n",
        "test_loss, test_acc = None, None\n",
        "\n",
        "# Evaluate on the test set\n",
        "try:\n",
        "    test_loss, test_acc = best_model.evaluate(X_test, verbose=1)\n",
        "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(\"Error during evaluation:\", e)\n",
        "    print(\"Ensure that the test data shape matches the input shape expected by the model.\")\n",
        "\n",
        "# Predict the emotions on the test data using the best model\n",
        "test_predictions = best_model.predict(X_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save the test predictions to a CSV file\n",
        "test_data['emotion'] = test_predicted_labels\n",
        "test_data[['id', 'emotion']].to_csv('mobilenet_predictions_9_FINAL.csv', index=False)\n",
        "print(\"Final predictions saved to mobilenet_predictions_9_FINAL.csv\")\n",
        "\n",
        "# Plot the test accuracy and loss if evaluation was successful\n",
        "def plot_test_performance(test_loss, test_acc):\n",
        "    if test_loss is not None and test_acc is not None:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.bar(['Test Accuracy', 'Test Loss'], [test_acc * 100, test_loss], color=['green', 'red'])\n",
        "        plt.title('Test Performance Metrics')\n",
        "        plt.ylabel('Percentage' if test_acc else 'Loss')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Test performance metrics not available due to an error during evaluation.\")\n",
        "\n",
        "# Call the plotting function to display test accuracy and loss\n",
        "plot_test_performance(test_loss, test_acc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6tR3JFhzo8",
        "outputId": "e813a68e-012a-4a1d-9148-9be6ffeb93e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during evaluation: None values not supported.\n",
            "Ensure that the test data shape matches the input shape expected by the model.\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "Final predictions saved to mobilenet_predictions_9_FINAL.csv\n",
            "Test performance metrics not available due to an error during evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model 2"
      ],
      "metadata": {
        "id": "h74hCYeLk91r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "def load_data(train_file, test_file):\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    return train_data, test_data\n",
        "\n",
        "def preprocess_data(data, is_train=True):\n",
        "    X = np.array([np.fromstring(pixels, dtype=int, sep=' ').reshape(48, 48, 1) for pixels in data['pixels']])\n",
        "    X = np.stack([X]*3, axis=-1)  # Convert to 3-channel (RGB)\n",
        "    X = X / 255.0  # Normalize pixel values\n",
        "    if is_train:\n",
        "        y = tf.keras.utils.to_categorical(data['emotion'], num_classes=7)\n",
        "        return X, y\n",
        "    return X\n",
        "\n",
        "# Load data\n",
        "train_file = '/content/train_dataset.csv.zip'\n",
        "test_file = '/content/test_dataset.csv.zip'\n",
        "train_data, test_data = load_data(train_file, test_file)\n",
        "\n",
        "# Preprocess train and test datasets\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test = preprocess_data(test_data, is_train=False)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Build the model using EfficientNetB0 as the base model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(7, activation='softmax')  # 7 emotion classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "csv_logger = CSVLogger('training.log')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "callbacks = [csv_logger, early_stop, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    data_generator.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('best_model.keras')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_acc = best_model.evaluate(X_test, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predict the emotions on the test data\n",
        "test_predictions = best_model.predict(X_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save the test predictions to a CSV file\n",
        "test_data['emotion'] = test_predicted_labels\n",
        "test_data[['id', 'emotion']].to_csv('efficientnet_predictions.csv', index=False)\n",
        "print(\"Predictions saved to efficientnet_predictions.csv\")\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "def plot_training(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function\n",
        "plot_training(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "2CAKoN9Lk-oL",
        "outputId": "f9e54366-5d69-4b78-8705-b40effab27a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape (4000, 48, 48, 1, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-431edbc78cf2>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m history = model.fit(\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     ):\n\u001b[0;32m-> 1103\u001b[0;31m         return NumpyArrayIterator(\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_misc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_misc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    613\u001b[0m                 \u001b[0;34m\"Input data in `NumpyArrayIterator` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;34m\"should have rank 4. You passed an array \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape (4000, 48, 48, 1, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "# Load and preprocess data from CSV\n",
        "def load_data(train_file, test_file):\n",
        "    train_data = pd.read_csv('/content/train_dataset.csv.zip')\n",
        "    test_data = pd.read_csv('/content/test_dataset.csv.zip')\n",
        "    return train_data, test_data\n",
        "\n",
        "def preprocess_data(data, is_train=True):\n",
        "    X = np.array([np.fromstring(pixels, dtype=int, sep=' ').reshape(48, 48, 1) for pixels in data['pixels']])\n",
        "    #X = np.concatenate([X]*3, axis=-1)  # Ensure data is in RGB format\n",
        "    X = X / 255.0\n",
        "    if is_train:\n",
        "        y = tf.keras.utils.to_categorical(data['emotion'], num_classes=7)\n",
        "        return X, y\n",
        "    return X\n",
        "\n",
        "# Load data\n",
        "train_file = '/path_to_your/train.csv'\n",
        "test_file = '/path_to_your/test.csv'\n",
        "train_data, test_data = load_data(train_file, test_file)\n",
        "\n",
        "# Preprocess train and test datasets\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test = preprocess_data(test_data, is_train=False)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "def train_data_generator():\n",
        "    while True:\n",
        "        for batch in data_generator.flow(X_train, y_train, batch_size=32):\n",
        "            yield batch\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    train_data_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32), # Changed from 3 to 1\n",
        "        tf.TensorSpec(shape=(None, 7), dtype=tf.float32)\n",
        "    )\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Rebuild the model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(48, 48, 3)) # This needs to be 3\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with the correct input shape\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "csv_logger = CSVLogger('training.log')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "callbacks = [csv_logger, early_stop, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=200,  # Increased epochs\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "try:\n",
        "    best_model = tf.keras.models.load_model('best_model.keras')\n",
        "except ValueError as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Handle or reconfigure the issue if needed\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_acc = best_model.evaluate(X_test, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predict the emotions on the test data\n",
        "test_predictions = best_model.predict(X_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save the test predictions to a CSV file\n",
        "test_data['emotion'] = test_predicted_labels\n",
        "test_data[['id', 'emotion']].to_csv('efficientnet_predictions.csv', index=False)\n",
        "print(\"Predictions saved to efficientnet_predictions.csv\")\n",
        "# Plot training and validation accuracy and loss\n",
        "def plot_training(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function\n",
        "plot_training(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H9kJdhZrw7Mq",
        "outputId": "36722d5c-f3b7-4341-b3a8-516c19b4c928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1694 - loss: 2.1751\n",
            "Epoch 1: val_loss improved from inf to 1.94079, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 118ms/step - accuracy: 0.1696 - loss: 2.1743 - val_accuracy: 0.2700 - val_loss: 1.9408 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.2217 - loss: 2.0052\n",
            "Epoch 2: val_loss did not improve from 1.94079\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2217 - loss: 2.0050 - val_accuracy: 0.2140 - val_loss: 1.9959 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2603 - loss: 1.8794\n",
            "Epoch 3: val_loss did not improve from 1.94079\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.2601 - loss: 1.8793 - val_accuracy: 0.1340 - val_loss: 2.0152 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2667 - loss: 1.8422\n",
            "Epoch 4: val_loss did not improve from 1.94079\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.2668 - loss: 1.8420 - val_accuracy: 0.1480 - val_loss: 1.9837 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2860 - loss: 1.7957\n",
            "Epoch 5: val_loss improved from 1.94079 to 1.89162, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.2860 - loss: 1.7955 - val_accuracy: 0.1820 - val_loss: 1.8916 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2996 - loss: 1.7573\n",
            "Epoch 6: val_loss improved from 1.89162 to 1.72891, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.2996 - loss: 1.7573 - val_accuracy: 0.3120 - val_loss: 1.7289 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3109 - loss: 1.7384\n",
            "Epoch 7: val_loss improved from 1.72891 to 1.64853, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.3109 - loss: 1.7384 - val_accuracy: 0.3460 - val_loss: 1.6485 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3257 - loss: 1.6716\n",
            "Epoch 8: val_loss improved from 1.64853 to 1.59728, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3257 - loss: 1.6717 - val_accuracy: 0.3810 - val_loss: 1.5973 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3109 - loss: 1.7074\n",
            "Epoch 9: val_loss improved from 1.59728 to 1.58869, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.3112 - loss: 1.7069 - val_accuracy: 0.3840 - val_loss: 1.5887 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3455 - loss: 1.6607\n",
            "Epoch 10: val_loss improved from 1.58869 to 1.57955, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.3455 - loss: 1.6607 - val_accuracy: 0.3760 - val_loss: 1.5795 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3492 - loss: 1.6554\n",
            "Epoch 11: val_loss improved from 1.57955 to 1.57923, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.3494 - loss: 1.6550 - val_accuracy: 0.3890 - val_loss: 1.5792 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3632 - loss: 1.6030\n",
            "Epoch 12: val_loss improved from 1.57923 to 1.56121, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.3633 - loss: 1.6031 - val_accuracy: 0.3890 - val_loss: 1.5612 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3682 - loss: 1.6134\n",
            "Epoch 13: val_loss improved from 1.56121 to 1.53908, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.3681 - loss: 1.6135 - val_accuracy: 0.4030 - val_loss: 1.5391 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3986 - loss: 1.5420\n",
            "Epoch 14: val_loss improved from 1.53908 to 1.52216, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.3983 - loss: 1.5426 - val_accuracy: 0.4120 - val_loss: 1.5222 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3724 - loss: 1.5950\n",
            "Epoch 15: val_loss improved from 1.52216 to 1.51557, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.3728 - loss: 1.5946 - val_accuracy: 0.4030 - val_loss: 1.5156 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4041 - loss: 1.5386\n",
            "Epoch 16: val_loss improved from 1.51557 to 1.48683, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.4040 - loss: 1.5390 - val_accuracy: 0.4180 - val_loss: 1.4868 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4011 - loss: 1.5478\n",
            "Epoch 17: val_loss did not improve from 1.48683\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.4012 - loss: 1.5478 - val_accuracy: 0.4250 - val_loss: 1.4934 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4138 - loss: 1.5098\n",
            "Epoch 18: val_loss improved from 1.48683 to 1.48218, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.4137 - loss: 1.5099 - val_accuracy: 0.4310 - val_loss: 1.4822 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4059 - loss: 1.5074\n",
            "Epoch 19: val_loss improved from 1.48218 to 1.47498, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.4059 - loss: 1.5074 - val_accuracy: 0.4480 - val_loss: 1.4750 - learning_rate: 1.0000e-04\n",
            "Epoch 20/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4221 - loss: 1.5076\n",
            "Epoch 20: val_loss improved from 1.47498 to 1.45710, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.4221 - loss: 1.5075 - val_accuracy: 0.4490 - val_loss: 1.4571 - learning_rate: 1.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4483 - loss: 1.4688\n",
            "Epoch 21: val_loss improved from 1.45710 to 1.42549, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.4482 - loss: 1.4689 - val_accuracy: 0.4540 - val_loss: 1.4255 - learning_rate: 1.0000e-04\n",
            "Epoch 22/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4373 - loss: 1.4691\n",
            "Epoch 22: val_loss improved from 1.42549 to 1.41921, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.4373 - loss: 1.4690 - val_accuracy: 0.4710 - val_loss: 1.4192 - learning_rate: 1.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4473 - loss: 1.4479\n",
            "Epoch 23: val_loss did not improve from 1.41921\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.4474 - loss: 1.4478 - val_accuracy: 0.4630 - val_loss: 1.4321 - learning_rate: 1.0000e-04\n",
            "Epoch 24/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4477 - loss: 1.4234\n",
            "Epoch 24: val_loss did not improve from 1.41921\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.4476 - loss: 1.4236 - val_accuracy: 0.4670 - val_loss: 1.4344 - learning_rate: 1.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4633 - loss: 1.4219\n",
            "Epoch 25: val_loss improved from 1.41921 to 1.41696, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.4632 - loss: 1.4220 - val_accuracy: 0.4830 - val_loss: 1.4170 - learning_rate: 1.0000e-04\n",
            "Epoch 26/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4606 - loss: 1.4395\n",
            "Epoch 26: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.4605 - loss: 1.4394 - val_accuracy: 0.4720 - val_loss: 1.4173 - learning_rate: 1.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4706 - loss: 1.3932\n",
            "Epoch 27: val_loss did not improve from 1.41696\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4706 - loss: 1.3935 - val_accuracy: 0.4610 - val_loss: 1.4254 - learning_rate: 1.0000e-04\n",
            "Epoch 28/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4650 - loss: 1.4039\n",
            "Epoch 28: val_loss improved from 1.41696 to 1.38891, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.4652 - loss: 1.4037 - val_accuracy: 0.4780 - val_loss: 1.3889 - learning_rate: 1.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4905 - loss: 1.3529\n",
            "Epoch 29: val_loss improved from 1.38891 to 1.37356, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.4904 - loss: 1.3530 - val_accuracy: 0.4740 - val_loss: 1.3736 - learning_rate: 1.0000e-04\n",
            "Epoch 30/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4828 - loss: 1.3689\n",
            "Epoch 30: val_loss improved from 1.37356 to 1.37221, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.4828 - loss: 1.3688 - val_accuracy: 0.4630 - val_loss: 1.3722 - learning_rate: 1.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4907 - loss: 1.3620\n",
            "Epoch 31: val_loss did not improve from 1.37221\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.4907 - loss: 1.3618 - val_accuracy: 0.4630 - val_loss: 1.3956 - learning_rate: 1.0000e-04\n",
            "Epoch 32/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4983 - loss: 1.3458\n",
            "Epoch 32: val_loss improved from 1.37221 to 1.36732, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.4982 - loss: 1.3460 - val_accuracy: 0.4800 - val_loss: 1.3673 - learning_rate: 1.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4866 - loss: 1.3171\n",
            "Epoch 33: val_loss did not improve from 1.36732\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.4866 - loss: 1.3172 - val_accuracy: 0.4800 - val_loss: 1.3688 - learning_rate: 1.0000e-04\n",
            "Epoch 34/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5003 - loss: 1.3187\n",
            "Epoch 34: val_loss did not improve from 1.36732\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5003 - loss: 1.3186 - val_accuracy: 0.4900 - val_loss: 1.3754 - learning_rate: 1.0000e-04\n",
            "Epoch 35/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5136 - loss: 1.2877\n",
            "Epoch 35: val_loss improved from 1.36732 to 1.36357, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5134 - loss: 1.2879 - val_accuracy: 0.4850 - val_loss: 1.3636 - learning_rate: 1.0000e-04\n",
            "Epoch 36/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4963 - loss: 1.3270\n",
            "Epoch 36: val_loss improved from 1.36357 to 1.35517, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.4965 - loss: 1.3267 - val_accuracy: 0.4830 - val_loss: 1.3552 - learning_rate: 1.0000e-04\n",
            "Epoch 37/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5078 - loss: 1.2886\n",
            "Epoch 37: val_loss improved from 1.35517 to 1.34726, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5078 - loss: 1.2885 - val_accuracy: 0.4780 - val_loss: 1.3473 - learning_rate: 1.0000e-04\n",
            "Epoch 38/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5143 - loss: 1.2742\n",
            "Epoch 38: val_loss did not improve from 1.34726\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5142 - loss: 1.2742 - val_accuracy: 0.4650 - val_loss: 1.3821 - learning_rate: 1.0000e-04\n",
            "Epoch 39/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5268 - loss: 1.2625\n",
            "Epoch 39: val_loss did not improve from 1.34726\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.5267 - loss: 1.2627 - val_accuracy: 0.4840 - val_loss: 1.3611 - learning_rate: 1.0000e-04\n",
            "Epoch 40/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5344 - loss: 1.2038\n",
            "Epoch 40: val_loss did not improve from 1.34726\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5342 - loss: 1.2044 - val_accuracy: 0.4910 - val_loss: 1.3695 - learning_rate: 1.0000e-04\n",
            "Epoch 41/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5446 - loss: 1.2221\n",
            "Epoch 41: val_loss improved from 1.34726 to 1.33968, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.5445 - loss: 1.2223 - val_accuracy: 0.5120 - val_loss: 1.3397 - learning_rate: 1.0000e-04\n",
            "Epoch 42/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5557 - loss: 1.1718\n",
            "Epoch 42: val_loss improved from 1.33968 to 1.33367, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.5554 - loss: 1.1723 - val_accuracy: 0.5070 - val_loss: 1.3337 - learning_rate: 1.0000e-04\n",
            "Epoch 43/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5374 - loss: 1.2228\n",
            "Epoch 43: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.5374 - loss: 1.2226 - val_accuracy: 0.5090 - val_loss: 1.3429 - learning_rate: 1.0000e-04\n",
            "Epoch 44/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5710 - loss: 1.1410\n",
            "Epoch 44: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5707 - loss: 1.1417 - val_accuracy: 0.4980 - val_loss: 1.3557 - learning_rate: 1.0000e-04\n",
            "Epoch 45/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5680 - loss: 1.1691\n",
            "Epoch 45: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.5678 - loss: 1.1694 - val_accuracy: 0.4810 - val_loss: 1.3622 - learning_rate: 1.0000e-04\n",
            "Epoch 46/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5627 - loss: 1.1635\n",
            "Epoch 46: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.5624 - loss: 1.1642 - val_accuracy: 0.5090 - val_loss: 1.3421 - learning_rate: 1.0000e-04\n",
            "Epoch 47/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5651 - loss: 1.1511\n",
            "Epoch 47: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 47: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5651 - loss: 1.1512 - val_accuracy: 0.5200 - val_loss: 1.3470 - learning_rate: 1.0000e-04\n",
            "Epoch 48/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5778 - loss: 1.1364\n",
            "Epoch 48: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5778 - loss: 1.1364 - val_accuracy: 0.5210 - val_loss: 1.3418 - learning_rate: 1.0000e-05\n",
            "Epoch 49/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5660 - loss: 1.1751\n",
            "Epoch 49: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.5661 - loss: 1.1749 - val_accuracy: 0.5150 - val_loss: 1.3397 - learning_rate: 1.0000e-05\n",
            "Epoch 50/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5818 - loss: 1.1165\n",
            "Epoch 50: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.5818 - loss: 1.1166 - val_accuracy: 0.5130 - val_loss: 1.3436 - learning_rate: 1.0000e-05\n",
            "Epoch 51/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5872 - loss: 1.1041\n",
            "Epoch 51: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.5871 - loss: 1.1042 - val_accuracy: 0.5100 - val_loss: 1.3435 - learning_rate: 1.0000e-05\n",
            "Epoch 52/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5773 - loss: 1.1130\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 52: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.5772 - loss: 1.1131 - val_accuracy: 0.5110 - val_loss: 1.3435 - learning_rate: 1.0000e-05\n",
            "Epoch 53/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5831 - loss: 1.1117\n",
            "Epoch 53: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.5830 - loss: 1.1118 - val_accuracy: 0.5100 - val_loss: 1.3445 - learning_rate: 1.0000e-06\n",
            "Epoch 54/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5793 - loss: 1.1372\n",
            "Epoch 54: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.5793 - loss: 1.1371 - val_accuracy: 0.5110 - val_loss: 1.3431 - learning_rate: 1.0000e-06\n",
            "Epoch 55/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5814 - loss: 1.1370\n",
            "Epoch 55: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.5814 - loss: 1.1370 - val_accuracy: 0.5140 - val_loss: 1.3420 - learning_rate: 1.0000e-06\n",
            "Epoch 56/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5799 - loss: 1.1151\n",
            "Epoch 56: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.5799 - loss: 1.1152 - val_accuracy: 0.5150 - val_loss: 1.3447 - learning_rate: 1.0000e-06\n",
            "Epoch 57/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5818 - loss: 1.1260\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "Epoch 57: val_loss did not improve from 1.33367\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.5817 - loss: 1.1260 - val_accuracy: 0.5140 - val_loss: 1.3451 - learning_rate: 1.0000e-06\n",
            "Error loading model: Layer \"dense_14\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_8224>, <KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_8225>]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "None values not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a35bc6365e96>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_acc*100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None values not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11/9/2024"
      ],
      "metadata": {
        "id": "AWnQ59Y18jPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data from CSV\n",
        "def load_data(train_file, test_file):\n",
        "    train_data = pd.read_csv(train_file)\n",
        "    test_data = pd.read_csv(test_file)\n",
        "    return train_data, test_data\n",
        "\n",
        "def preprocess_data(data, is_train=True):\n",
        "    X = np.array([np.fromstring(pixels, dtype=int, sep=' ').reshape(48, 48, 1) for pixels in data['pixels']])\n",
        "    X = np.repeat(X, 3, axis=-1)  # Convert to 3-channel (RGB)\n",
        "    X = X / 255.0  # Normalize pixel values\n",
        "    if is_train:\n",
        "        y = tf.keras.utils.to_categorical(data['emotion'], num_classes=7)\n",
        "        return X, y\n",
        "    return X\n",
        "\n",
        "# Load data\n",
        "train_file = '/content/train_dataset.csv.zip'\n",
        "test_file = '/content/test_dataset.csv.zip'\n",
        "train_data, test_data = load_data(train_file, test_file)\n",
        "\n",
        "# Preprocess train and test datasets\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test = preprocess_data(test_data, is_train=False)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Rebuild the model with correct input shape for EfficientNet\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(48, 48, 3))  # 3-channel input\n",
        "\n",
        "# Ensure to only pass the output of the base model to the subsequent layers\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(7, activation='softmax')  # 7 emotion classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "csv_logger = CSVLogger('training.log')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "callbacks = [csv_logger, early_stop, reduce_lr, model_checkpoint]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    data_generator.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=200,\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('best_model.keras')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_acc = best_model.evaluate(X_test, verbose=1)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Predict the emotions on the test data\n",
        "test_predictions = best_model.predict(X_test)\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save the test predictions to a CSV file\n",
        "test_data['emotion'] = test_predicted_labels\n",
        "test_data[['id', 'emotion']].to_csv('efficientnet_predictions.csv', index=False)\n",
        "print(\"Predictions saved to efficientnet_predictions.csv\")\n",
        "\n",
        "# Plot training and validation accuracy and loss\n",
        "def plot_training(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Call the plotting function\n",
        "plot_training(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_7pVSFCVACUh",
        "outputId": "c735dcaa-56be-435a-e4bd-295489d73e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1895 - loss: 2.1238\n",
            "Epoch 1: val_loss improved from inf to 2.16684, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 151ms/step - accuracy: 0.1895 - loss: 2.1227 - val_accuracy: 0.1540 - val_loss: 2.1668 - learning_rate: 1.0000e-04\n",
            "Epoch 2/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss did not improve from 2.16684\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1540 - val_loss: 2.1668 - learning_rate: 1.0000e-04\n",
            "Epoch 3/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2164 - loss: 1.9685\n",
            "Epoch 3: val_loss improved from 2.16684 to 2.05234, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - accuracy: 0.2164 - loss: 1.9683 - val_accuracy: 0.1290 - val_loss: 2.0523 - learning_rate: 1.0000e-04\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 4: val_loss did not improve from 2.05234\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1290 - val_loss: 2.0523 - learning_rate: 1.0000e-04\n",
            "Epoch 5/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2476 - loss: 1.9033\n",
            "Epoch 5: val_loss improved from 2.05234 to 1.97438, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 52ms/step - accuracy: 0.2476 - loss: 1.9030 - val_accuracy: 0.1500 - val_loss: 1.9744 - learning_rate: 1.0000e-04\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 6: val_loss did not improve from 1.97438\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1500 - val_loss: 1.9744 - learning_rate: 1.0000e-04\n",
            "Epoch 7/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2662 - loss: 1.8354\n",
            "Epoch 7: val_loss improved from 1.97438 to 1.96593, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.2664 - loss: 1.8350 - val_accuracy: 0.1610 - val_loss: 1.9659 - learning_rate: 1.0000e-04\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.96593\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1610 - val_loss: 1.9659 - learning_rate: 1.0000e-04\n",
            "Epoch 9/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.2834 - loss: 1.8031\n",
            "Epoch 9: val_loss improved from 1.96593 to 1.91326, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.2835 - loss: 1.8029 - val_accuracy: 0.1720 - val_loss: 1.9133 - learning_rate: 1.0000e-04\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.91326\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.1720 - val_loss: 1.9133 - learning_rate: 1.0000e-04\n",
            "Epoch 11/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3020 - loss: 1.7564\n",
            "Epoch 11: val_loss improved from 1.91326 to 1.76130, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.3020 - loss: 1.7564 - val_accuracy: 0.2870 - val_loss: 1.7613 - learning_rate: 1.0000e-04\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 12: val_loss did not improve from 1.76130\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.2870 - val_loss: 1.7613 - learning_rate: 1.0000e-04\n",
            "Epoch 13/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.2967 - loss: 1.7376\n",
            "Epoch 13: val_loss improved from 1.76130 to 1.69849, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.2967 - loss: 1.7376 - val_accuracy: 0.3420 - val_loss: 1.6985 - learning_rate: 1.0000e-04\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 14: val_loss did not improve from 1.69849\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3420 - val_loss: 1.6985 - learning_rate: 1.0000e-04\n",
            "Epoch 15/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3272 - loss: 1.7260\n",
            "Epoch 15: val_loss improved from 1.69849 to 1.59189, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - accuracy: 0.3275 - loss: 1.7257 - val_accuracy: 0.3840 - val_loss: 1.5919 - learning_rate: 1.0000e-04\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 16: val_loss did not improve from 1.59189\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3840 - val_loss: 1.5919 - learning_rate: 1.0000e-04\n",
            "Epoch 17/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3284 - loss: 1.6744\n",
            "Epoch 17: val_loss improved from 1.59189 to 1.56056, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.3285 - loss: 1.6743 - val_accuracy: 0.3940 - val_loss: 1.5606 - learning_rate: 1.0000e-04\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 18: val_loss did not improve from 1.56056\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3940 - val_loss: 1.5606 - learning_rate: 1.0000e-04\n",
            "Epoch 19/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3517 - loss: 1.6526\n",
            "Epoch 19: val_loss improved from 1.56056 to 1.55150, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.3518 - loss: 1.6526 - val_accuracy: 0.4200 - val_loss: 1.5515 - learning_rate: 1.0000e-04\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 20: val_loss did not improve from 1.55150\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4200 - val_loss: 1.5515 - learning_rate: 1.0000e-04\n",
            "Epoch 21/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3534 - loss: 1.6396\n",
            "Epoch 21: val_loss did not improve from 1.55150\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.3534 - loss: 1.6394 - val_accuracy: 0.3970 - val_loss: 1.5598 - learning_rate: 1.0000e-04\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 22: val_loss did not improve from 1.55150\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.3970 - val_loss: 1.5598 - learning_rate: 1.0000e-04\n",
            "Epoch 23/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3921 - loss: 1.5828\n",
            "Epoch 23: val_loss improved from 1.55150 to 1.54624, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 52ms/step - accuracy: 0.3919 - loss: 1.5830 - val_accuracy: 0.4070 - val_loss: 1.5462 - learning_rate: 1.0000e-04\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 24: val_loss did not improve from 1.54624\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4070 - val_loss: 1.5462 - learning_rate: 1.0000e-04\n",
            "Epoch 25/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3768 - loss: 1.6053\n",
            "Epoch 25: val_loss improved from 1.54624 to 1.50490, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.3769 - loss: 1.6052 - val_accuracy: 0.4490 - val_loss: 1.5049 - learning_rate: 1.0000e-04\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 26: val_loss did not improve from 1.50490\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4490 - val_loss: 1.5049 - learning_rate: 1.0000e-04\n",
            "Epoch 27/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3839 - loss: 1.5973\n",
            "Epoch 27: val_loss improved from 1.50490 to 1.48866, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 60ms/step - accuracy: 0.3839 - loss: 1.5970 - val_accuracy: 0.4610 - val_loss: 1.4887 - learning_rate: 1.0000e-04\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 28: val_loss did not improve from 1.48866\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4610 - val_loss: 1.4887 - learning_rate: 1.0000e-04\n",
            "Epoch 29/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3811 - loss: 1.5850\n",
            "Epoch 29: val_loss improved from 1.48866 to 1.47786, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.3812 - loss: 1.5849 - val_accuracy: 0.4380 - val_loss: 1.4779 - learning_rate: 1.0000e-04\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.47786\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4380 - val_loss: 1.4779 - learning_rate: 1.0000e-04\n",
            "Epoch 31/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3948 - loss: 1.5648\n",
            "Epoch 31: val_loss improved from 1.47786 to 1.47322, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 52ms/step - accuracy: 0.3949 - loss: 1.5644 - val_accuracy: 0.4450 - val_loss: 1.4732 - learning_rate: 1.0000e-04\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 32: val_loss did not improve from 1.47322\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4450 - val_loss: 1.4732 - learning_rate: 1.0000e-04\n",
            "Epoch 33/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4080 - loss: 1.5143\n",
            "Epoch 33: val_loss improved from 1.47322 to 1.46203, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.4079 - loss: 1.5145 - val_accuracy: 0.4540 - val_loss: 1.4620 - learning_rate: 1.0000e-04\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 34: val_loss did not improve from 1.46203\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4540 - val_loss: 1.4620 - learning_rate: 1.0000e-04\n",
            "Epoch 35/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4185 - loss: 1.4972\n",
            "Epoch 35: val_loss did not improve from 1.46203\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 56ms/step - accuracy: 0.4183 - loss: 1.4976 - val_accuracy: 0.4300 - val_loss: 1.4643 - learning_rate: 1.0000e-04\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 36: val_loss did not improve from 1.46203\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4300 - val_loss: 1.4643 - learning_rate: 1.0000e-04\n",
            "Epoch 37/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4030 - loss: 1.5324\n",
            "Epoch 37: val_loss improved from 1.46203 to 1.44709, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 0.4031 - loss: 1.5321 - val_accuracy: 0.4550 - val_loss: 1.4471 - learning_rate: 1.0000e-04\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 38: val_loss did not improve from 1.44709\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4550 - val_loss: 1.4471 - learning_rate: 1.0000e-04\n",
            "Epoch 39/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4295 - loss: 1.4880\n",
            "Epoch 39: val_loss improved from 1.44709 to 1.43000, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.4294 - loss: 1.4880 - val_accuracy: 0.4610 - val_loss: 1.4300 - learning_rate: 1.0000e-04\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 40: val_loss did not improve from 1.43000\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4610 - val_loss: 1.4300 - learning_rate: 1.0000e-04\n",
            "Epoch 41/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4337 - loss: 1.4796\n",
            "Epoch 41: val_loss improved from 1.43000 to 1.41949, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 62ms/step - accuracy: 0.4338 - loss: 1.4795 - val_accuracy: 0.4580 - val_loss: 1.4195 - learning_rate: 1.0000e-04\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 42: val_loss did not improve from 1.41949\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4580 - val_loss: 1.4195 - learning_rate: 1.0000e-04\n",
            "Epoch 43/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4422 - loss: 1.4473\n",
            "Epoch 43: val_loss improved from 1.41949 to 1.39318, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.4424 - loss: 1.4473 - val_accuracy: 0.4680 - val_loss: 1.3932 - learning_rate: 1.0000e-04\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 44: val_loss did not improve from 1.39318\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4680 - val_loss: 1.3932 - learning_rate: 1.0000e-04\n",
            "Epoch 45/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4507 - loss: 1.4286\n",
            "Epoch 45: val_loss did not improve from 1.39318\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 0.4507 - loss: 1.4287 - val_accuracy: 0.4710 - val_loss: 1.4117 - learning_rate: 1.0000e-04\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 46: val_loss did not improve from 1.39318\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4710 - val_loss: 1.4117 - learning_rate: 1.0000e-04\n",
            "Epoch 47/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4729 - loss: 1.4039\n",
            "Epoch 47: val_loss improved from 1.39318 to 1.38465, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - accuracy: 0.4728 - loss: 1.4041 - val_accuracy: 0.4730 - val_loss: 1.3847 - learning_rate: 1.0000e-04\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 48: val_loss did not improve from 1.38465\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4730 - val_loss: 1.3847 - learning_rate: 1.0000e-04\n",
            "Epoch 49/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4761 - loss: 1.4225\n",
            "Epoch 49: val_loss improved from 1.38465 to 1.37387, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.4758 - loss: 1.4227 - val_accuracy: 0.4730 - val_loss: 1.3739 - learning_rate: 1.0000e-04\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 50: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4730 - val_loss: 1.3739 - learning_rate: 1.0000e-04\n",
            "Epoch 51/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4671 - loss: 1.3875\n",
            "Epoch 51: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.4671 - loss: 1.3876 - val_accuracy: 0.4780 - val_loss: 1.3921 - learning_rate: 1.0000e-04\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 52: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4780 - val_loss: 1.3921 - learning_rate: 1.0000e-04\n",
            "Epoch 53/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4666 - loss: 1.3981\n",
            "Epoch 53: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.4664 - loss: 1.3982 - val_accuracy: 0.4880 - val_loss: 1.3826 - learning_rate: 1.0000e-04\n",
            "Epoch 54/200\n",
            "\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 54: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4880 - val_loss: 1.3826 - learning_rate: 1.0000e-04\n",
            "Epoch 55/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4583 - loss: 1.3967\n",
            "Epoch 55: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 54ms/step - accuracy: 0.4583 - loss: 1.3966 - val_accuracy: 0.4800 - val_loss: 1.3784 - learning_rate: 1.0000e-05\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 56: val_loss did not improve from 1.37387\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4800 - val_loss: 1.3784 - learning_rate: 1.0000e-05\n",
            "Epoch 57/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4703 - loss: 1.3889\n",
            "Epoch 57: val_loss improved from 1.37387 to 1.37301, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.4705 - loss: 1.3885 - val_accuracy: 0.4790 - val_loss: 1.3730 - learning_rate: 1.0000e-05\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 58: val_loss did not improve from 1.37301\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4790 - val_loss: 1.3730 - learning_rate: 1.0000e-05\n",
            "Epoch 59/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4785 - loss: 1.3588\n",
            "Epoch 59: val_loss did not improve from 1.37301\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - accuracy: 0.4785 - loss: 1.3587 - val_accuracy: 0.4830 - val_loss: 1.3754 - learning_rate: 1.0000e-05\n",
            "Epoch 60/200\n",
            "\n",
            "Epoch 60: val_loss did not improve from 1.37301\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4830 - val_loss: 1.3754 - learning_rate: 1.0000e-05\n",
            "Epoch 61/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4727 - loss: 1.3719\n",
            "Epoch 61: val_loss improved from 1.37301 to 1.36862, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.4727 - loss: 1.3719 - val_accuracy: 0.4800 - val_loss: 1.3686 - learning_rate: 1.0000e-05\n",
            "Epoch 62/200\n",
            "\n",
            "Epoch 62: val_loss did not improve from 1.36862\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4800 - val_loss: 1.3686 - learning_rate: 1.0000e-05\n",
            "Epoch 63/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4837 - loss: 1.3609\n",
            "Epoch 63: val_loss improved from 1.36862 to 1.36728, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.4837 - loss: 1.3609 - val_accuracy: 0.4800 - val_loss: 1.3673 - learning_rate: 1.0000e-05\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 64: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4800 - val_loss: 1.3673 - learning_rate: 1.0000e-05\n",
            "Epoch 65/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.4722 - loss: 1.3604\n",
            "Epoch 65: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - accuracy: 0.4723 - loss: 1.3604 - val_accuracy: 0.4810 - val_loss: 1.3689 - learning_rate: 1.0000e-05\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 66: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4810 - val_loss: 1.3689 - learning_rate: 1.0000e-05\n",
            "Epoch 67/200\n",
            "\u001b[1m123/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5069 - loss: 1.3293\n",
            "Epoch 67: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.5068 - loss: 1.3296 - val_accuracy: 0.4820 - val_loss: 1.3682 - learning_rate: 1.0000e-05\n",
            "Epoch 68/200\n",
            "\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 68: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4820 - val_loss: 1.3682 - learning_rate: 1.0000e-05\n",
            "Epoch 69/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4887 - loss: 1.3355\n",
            "Epoch 69: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.4887 - loss: 1.3356 - val_accuracy: 0.4800 - val_loss: 1.3683 - learning_rate: 1.0000e-06\n",
            "Epoch 70/200\n",
            "\n",
            "Epoch 70: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4800 - val_loss: 1.3683 - learning_rate: 1.0000e-06\n",
            "Epoch 71/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5043 - loss: 1.3199\n",
            "Epoch 71: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 46ms/step - accuracy: 0.5043 - loss: 1.3200 - val_accuracy: 0.4810 - val_loss: 1.3683 - learning_rate: 1.0000e-06\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 72: val_loss did not improve from 1.36728\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4810 - val_loss: 1.3683 - learning_rate: 1.0000e-06\n",
            "Epoch 73/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5025 - loss: 1.3217\n",
            "Epoch 73: val_loss improved from 1.36728 to 1.36573, saving model to best_model.keras\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.5024 - loss: 1.3219 - val_accuracy: 0.4770 - val_loss: 1.3657 - learning_rate: 1.0000e-06\n",
            "Epoch 74/200\n",
            "\n",
            "Epoch 74: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4770 - val_loss: 1.3657 - learning_rate: 1.0000e-06\n",
            "Epoch 75/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4802 - loss: 1.3517\n",
            "Epoch 75: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 55ms/step - accuracy: 0.4803 - loss: 1.3516 - val_accuracy: 0.4780 - val_loss: 1.3676 - learning_rate: 1.0000e-06\n",
            "Epoch 76/200\n",
            "\n",
            "Epoch 76: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4780 - val_loss: 1.3676 - learning_rate: 1.0000e-06\n",
            "Epoch 77/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4772 - loss: 1.3432\n",
            "Epoch 77: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.4774 - loss: 1.3430 - val_accuracy: 0.4800 - val_loss: 1.3677 - learning_rate: 1.0000e-06\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "Epoch 78: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4800 - val_loss: 1.3677 - learning_rate: 1.0000e-06\n",
            "Epoch 79/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4706 - loss: 1.3784\n",
            "Epoch 79: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.4708 - loss: 1.3781 - val_accuracy: 0.4790 - val_loss: 1.3678 - learning_rate: 1.0000e-07\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 80: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4790 - val_loss: 1.3678 - learning_rate: 1.0000e-07\n",
            "Epoch 81/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4795 - loss: 1.3510\n",
            "Epoch 81: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 45ms/step - accuracy: 0.4796 - loss: 1.3509 - val_accuracy: 0.4790 - val_loss: 1.3661 - learning_rate: 1.0000e-07\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 82: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4790 - val_loss: 1.3661 - learning_rate: 1.0000e-07\n",
            "Epoch 83/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4877 - loss: 1.3445\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "Epoch 83: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.4877 - loss: 1.3446 - val_accuracy: 0.4780 - val_loss: 1.3682 - learning_rate: 1.0000e-07\n",
            "Epoch 84/200\n",
            "\n",
            "Epoch 84: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4780 - val_loss: 1.3682 - learning_rate: 1.0000e-08\n",
            "Epoch 85/200\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.4847 - loss: 1.3601\n",
            "Epoch 85: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.4847 - loss: 1.3602 - val_accuracy: 0.4780 - val_loss: 1.3678 - learning_rate: 1.0000e-08\n",
            "Epoch 86/200\n",
            "\n",
            "Epoch 86: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4780 - val_loss: 1.3678 - learning_rate: 1.0000e-08\n",
            "Epoch 87/200\n",
            "\u001b[1m124/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4965 - loss: 1.3373\n",
            "Epoch 87: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.4963 - loss: 1.3375 - val_accuracy: 0.4790 - val_loss: 1.3671 - learning_rate: 1.0000e-08\n",
            "Epoch 88/200\n",
            "\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "\n",
            "Epoch 88: val_loss did not improve from 1.36573\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4790 - val_loss: 1.3671 - learning_rate: 1.0000e-08\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"dense_2\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_1498>, <KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_1499>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6c5d91a0623a>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Evaluate on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 )\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         if (\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_rebuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36m_maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_lock_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mbuild_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0moriginal_build_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0;31m# Record build config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_build_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Can happen if shape inference is not implemented.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"dense_2\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_1498>, <KerasTensor shape=(None, 2, 2, 1280), dtype=float32, sparse=False, name=keras_tensor_1499>]"
          ]
        }
      ]
    }
  ]
}